{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入: torch.Size([1, 1, 2197])\n",
      "输出: torch.Size([1, 2197])\n",
      "第一次输出: tensor([[-0.0111,  0.0593, -0.0235,  ...,  0.0436, -0.0385, -0.1238]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "h_n: torch.Size([2, 1, 128])\n",
      "c_n: torch.Size([2, 1, 128])\n",
      "输入: torch.Size([1, 1, 2197])\n",
      "输出: torch.Size([1, 2197])\n",
      "第一次输出: tensor([[ 0.0033,  0.0623, -0.0255,  ...,  0.0506, -0.0307, -0.1387]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "h_n: torch.Size([2, 1, 128])\n",
      "c_n: torch.Size([2, 1, 128])\n",
      "输入: torch.Size([1, 1, 2197])\n",
      "输出: torch.Size([1, 2197])\n",
      "第3次输出: tensor([[-0.0026,  0.0404, -0.0338,  ...,  0.0430, -0.0272, -0.1315]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "输入: torch.Size([1, 1, 2197])\n",
      "输出: torch.Size([1, 2197])\n",
      "第4次输出: tensor([[ 0.0039,  0.0343, -0.0273,  ...,  0.0455, -0.0167, -0.1245]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "输入: torch.Size([1, 1, 2197])\n",
      "输出: torch.Size([1, 2197])\n",
      "第5次输出: tensor([[ 0.0062,  0.0417, -0.0219,  ...,  0.0495, -0.0286, -0.1408]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "输入: torch.Size([1, 1, 2197])\n",
      "输出: torch.Size([1, 2197])\n",
      "第6次输出: tensor([[-0.0097,  0.0206, -0.0167,  ...,  0.0588, -0.0147, -0.1512]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "输入: torch.Size([1, 1, 2197])\n",
      "输出: torch.Size([1, 2197])\n",
      "第7次输出: tensor([[-0.0197,  0.0198, -0.0215,  ...,  0.0537, -0.0243, -0.1380]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "LSTM_NEUROES = 13*13*13\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=LSTM_NEUROES, hidden_size=128, num_layers=2, output_size=LSTM_NEUROES):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.input_fc = nn.Linear(input_size + output_size, input_size)  # 映射到 input_size\n",
    "\n",
    "    def forward(self, x, h_0, c_0):\n",
    "        print(\"输入:\", x.shape)\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        out = out[:, -1, :]  # 取LSTM最后一个时间步的输出\n",
    "        out = self.fc(out)\n",
    "        print(\"输出:\", out.shape)\n",
    "        return out, h_n, c_n\n",
    "\n",
    "# 初始化模型\n",
    "model = LSTM()\n",
    "batch_size = 1\n",
    "\n",
    "# 初始化隐藏层状态\n",
    "h_0 = torch.zeros(model.num_layers, batch_size, model.hidden_size)\n",
    "c_0 = torch.zeros(model.num_layers, batch_size, model.hidden_size)\n",
    "\n",
    "# 第一次输入\n",
    "input_1 = torch.randn(batch_size, 1, LSTM_NEUROES)  # (batch_size, seq_len, input_size)\n",
    "output, h_n, c_n = model(input_1, h_0, c_0)\n",
    "print(\"第一次输出:\", output)\n",
    "print(\"h_n:\", h_n.shape)\n",
    "print(\"c_n:\", c_n.shape)\n",
    "\n",
    "# 第二次输入，使用第一次的输出作为一部分输入\n",
    "input_2 = torch.randn(batch_size, 1, LSTM_NEUROES)  # 另外的输入\n",
    "new_input = torch.cat((input_2, output.unsqueeze(1)), dim=2)  # 合并输出作为输入的一部分\n",
    "new_input = model.input_fc(new_input)  # 映射到原始input_size\n",
    "\n",
    "output, h_n, c_n = model(new_input, h_n, c_n)\n",
    "print(\"第一次输出:\", output)\n",
    "print(\"h_n:\", h_n.shape)\n",
    "print(\"c_n:\", c_n.shape)\n",
    "# 多次迭代输入进行修正\n",
    "num_iterations = 5\n",
    "for i in range(num_iterations):\n",
    "    input_next = torch.randn(batch_size, 1, LSTM_NEUROES)  # 另外的输入\n",
    "    new_input = torch.cat((input_next, output.unsqueeze(1)), dim=2)  # 合并输出作为输入的一部分\n",
    "    new_input = model.input_fc(new_input)  # 映射到原始input_size\n",
    "\n",
    "    output, h_n, c_n = model(new_input, h_n, c_n)\n",
    "    print(f\"第{i+3}次输出:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n",
      "input_t: torch.Size([1, 2197])\n",
      "out: torch.Size([1, 2197]), h_0: torch.Size([2, 1, 128]), c_0: torch.Size([2, 1, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_426916/261362166.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "LSTM_NEUROES = 13 * 13 * 13\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=LSTM_NEUROES, hidden_size=128, num_layers=2, output_size=LSTM_NEUROES):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.input_fc = nn.Linear(input_size + output_size, input_size)  # 映射到 input_size\n",
    "\n",
    "    def forward(self, input_t, prev_output, h_0, c_0):\n",
    "        if prev_output is not None:\n",
    "            input_t = torch.cat((input_t, prev_output), dim=1)\n",
    "            input_t = self.input_fc(input_t)  # 映射到原始input_size\n",
    "\n",
    "        input_t = input_t.unsqueeze(1)  # 调整形状为 (batch_size, seq_len, input_size)\n",
    "        out, (h_0, c_0) = self.lstm(input_t, (h_0, c_0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, h_0, c_0\n",
    "\n",
    "# 定义数据集\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_samples, seq_len, input_size):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.input_size = input_size\n",
    "        self.data = torch.randn(num_samples, seq_len, input_size)\n",
    "        self.targets = torch.randn(num_samples, input_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "num_samples = 1000\n",
    "seq_len = 5  # 假设序列长度为5\n",
    "dataset = CustomDataset(num_samples, seq_len, LSTM_NEUROES)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model = LSTM()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 20\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for inputs, targets in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        h_0 = torch.zeros(model.num_layers, batch_size, model.hidden_size).to(inputs.device)\n",
    "        c_0 = torch.zeros(model.num_layers, batch_size, model.hidden_size).to(inputs.device)\n",
    "        prev_output = None\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            input_t = inputs[:, t, :]\n",
    "            print(\"input_t:\", input_t.shape)\n",
    "            output, h_0, c_0 = model(input_t, prev_output, h_0, c_0)\n",
    "            print(\"out: {}, h_0: {}, c_0: {}\".format(output.shape, h_0.shape, c_0.shape))\n",
    "            prev_output = output\n",
    "\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(data_loader)}')\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'lstm_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
