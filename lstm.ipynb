{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from lib.checkpoint import load_checkpoint, save_checkpoint\n",
    "from lib.image import load_encoded_data\n",
    "from lib.binvox import load_voxel_file\n",
    "from lib.config import *\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_NEUROES = 13*13*13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            return False\n",
    "\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=LSTM_NEUROES, hidden_size=128, num_layers=2, output_size=LSTM_NEUROES):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.input_fc = nn.Linear(input_size + output_size, input_size)  # 映射到 input_size\n",
    "\n",
    "    def forward(self, input_t, prev_output, h_0, c_0):\n",
    "        if prev_output is not None:\n",
    "            input_t = torch.cat((input_t, prev_output), dim=1)\n",
    "            input_t = self.input_fc(input_t)  # 映射到原始input_size\n",
    "\n",
    "        input_t = input_t.unsqueeze(1)  # 调整形状为 (batch_size, seq_len, input_size)\n",
    "        out, (h_0, c_0) = self.lstm(input_t, (h_0, c_0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, h_0, c_0\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTNN3DDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CTNN3DDecoder, self).__init__()\n",
    "        self.fc = nn.Linear(LSTM_NEUROES, 8*8*8*16)\n",
    "        \n",
    "        self.cov1 = nn.ConvTranspose3d(in_channels = 16, out_channels = 14,\n",
    "                                kernel_size = 3, stride = 2, padding = 2)\n",
    "        self.cov2 = nn.ConvTranspose3d(in_channels = 14, out_channels = 12,\n",
    "                                kernel_size = 3, stride = 2, padding = 2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.cov3 = nn.ConvTranspose3d(in_channels = 12, out_channels = 10,\n",
    "                                kernel_size = 3, stride = 2, padding = 2)\n",
    "        self.cov4 = nn.ConvTranspose3d(in_channels = 10, out_channels = 8,\n",
    "                                kernel_size = 3, stride = 2, padding = 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "                \n",
    "        self.cov5 = nn.ConvTranspose3d(in_channels = 8, out_channels = 6,\n",
    "                                kernel_size = 3, stride = 2, padding = 2)\n",
    "        self.cov6 = nn.ConvTranspose3d(in_channels = 6, out_channels = 4,\n",
    "                                kernel_size = 3, stride = 2, padding = 2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.upsample = nn.Upsample(size=(32, 32, 32), mode='nearest')        \n",
    "        \n",
    "        # self.cov4 = nn.Conv3d(in_channels = 40, out_channel = 1, )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = out.view(1, 16, 8, 8, 8)\n",
    "        \n",
    "        out = self.cov1(out)\n",
    "        out = self.cov2(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.cov3(out)\n",
    "        out = self.cov4(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        out = self.cov5(out)\n",
    "        out = self.cov6(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.upsample(out)\n",
    "        return out\n",
    "        \n",
    "# model = CTNN3DDecoder()\n",
    "# test = torch.randn(LSTM_NEUROES)\n",
    "# model(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.lstm = LSTM()\n",
    "        self.decoder = CTNN3DDecoder()\n",
    "        \n",
    "    def forward(self, x, prev_output, h_0, c_0):\n",
    "        prev_out, h_0, c_0 = self.lstm(x, prev_output, h_0, c_0)\n",
    "        out = self.decoder(prev_out)\n",
    "        return out, prev_out, h_0, c_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, datas, voxel):\n",
    "        self.datas = datas\n",
    "        self.voxel = voxel\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.datas[index], self.voxel[index].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sub_epoch(epoch, datas, model, criterion, optimizer, device):\n",
    "    data_len = len(datas)\n",
    "    train_data, val_data = random_split(datas, [int(data_len * 0.8), data_len - int(data_len * 0.8)])\n",
    "    print(\"Epoch:{} Train Size:{} Val Size:{}\".format(epoch, len(train_data), len(val_data)))\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=1, shuffle=True)\n",
    "    train_logs = []\n",
    "    val_logs = []\n",
    "    # move data and model to device\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    seq_len = 1\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "        h_0 = torch.zeros(model.lstm.num_layers, batch_size, model.lstm.hidden_size).to(inputs.device)\n",
    "        c_0 = torch.zeros(model.lstm.num_layers, batch_size, model.lstm.hidden_size).to(inputs.device)\n",
    "        prev_output = None\n",
    "\n",
    "        inputs = inputs.view(batch_size, seq_len, -1)\n",
    "        for t in range(seq_len):\n",
    "            input_t = inputs[:, t]\n",
    "            decode_output, prev_output, h_0, c_0 = model(input_t, prev_output, h_0, c_0)\n",
    "        \n",
    "        test_chennal = decode_output[:, 0]\n",
    "        loss = criterion(test_chennal , targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_logs.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            h_0 = torch.zeros(model.lstm.num_layers, batch_size, model.lstm.hidden_size).to(inputs.device)\n",
    "            c_0 = torch.zeros(model.lstm.num_layers, batch_size, model.lstm.hidden_size).to(inputs.device)\n",
    "            prev_output = None\n",
    "\n",
    "            inputs = inputs.view(batch_size, seq_len, -1)\n",
    "            for t in range(seq_len):\n",
    "                input_t = inputs[:, t]\n",
    "                decode_output, prev_output, h_0, c_0 = model(input_t, prev_output, h_0, c_0)\n",
    "        \n",
    "            test_chennal = decode_output[:, 0]\n",
    "            loss = criterion(test_chennal , targets)\n",
    "            val_logs.append(loss.item())\n",
    "            \n",
    "    end = time.time()\n",
    "\n",
    "    print(\"Epoch:{} Sub Train Time:{:.2f} Train Loss:{:.4f} Val Loss:{:.4f}\".format(epoch, end - start,\n",
    "                                                                                    sum(train_logs) / len(train_logs),\n",
    "                                                                                    sum(val_logs) / len(val_logs)))\n",
    "    return sum(train_logs) / len(train_logs), sum(val_logs) / len(val_logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(file_path, device, checkpoint_path):\n",
    "    model = LSTMDecoder()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    early_stopping = EarlyStopping(patience=5, min_delta=0.0001)\n",
    "    epoch_losses = []\n",
    "    num_epochs = 20\n",
    "    \n",
    "    start_epoch, epoch_losses, last_folder, train_loss, val_loss = load_checkpoint(checkpoint_path, model, optimizer, device)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        start = time.time()\n",
    "        cnt = 0\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        resume = (last_folder is not None)\n",
    "        skip_cnt = 0\n",
    "        renders = []\n",
    "        voxels = []\n",
    "        for root, dirs, files in os.walk(\"dataset/\"):\n",
    "            folder = root.split(\"/\")[-1]\n",
    "            if(resume):\n",
    "                if(folder == last_folder):\n",
    "                    print(\"Resume from {}, Skip {} Files\".format(folder, skip_cnt))\n",
    "                    resume = False\n",
    "                else:\n",
    "                    skip_cnt += 1\n",
    "                    continue\n",
    "            start_io = time.time()\n",
    "            if(folder == \"\"): continue\n",
    "            print(root)\n",
    "            render = load_encoded_data(root)\n",
    "            voxel = load_voxel_file(root + \"/voxel.txt\")\n",
    "            voxel.reshape(32, 32, 32)\n",
    "            if render is None or voxel is None:\n",
    "                continue             \n",
    "            for i in render:\n",
    "                renders.append(i)\n",
    "                voxels.append(voxel)\n",
    "            cnt += 1\n",
    "            if(cnt >= 10):\n",
    "                end_io = time.time()\n",
    "                print(\"IO Time:{:.2f}\".format(end_io-start_io))\n",
    "                dataset = TrainDataset(renders, voxels)\n",
    "                train, val = train_sub_epoch(epoch, dataset, model, criterion, optimizer, device)\n",
    "                train_loss.append(train)\n",
    "                val_loss.append(val)\n",
    "                renders = []\n",
    "                voxels = []\n",
    "                cnt = 0\n",
    "                start_io = time.time()\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch_losses': epoch_losses,\n",
    "                    'last_file': folder,\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                }, filename = checkpoint_path)\n",
    "        if(cnt > 0):\n",
    "            dataset = TrainDataset(renders, voxels)\n",
    "            train, val = train_sub_epoch(epoch, dataset, model, criterion, optimizer, device)\n",
    "            train_loss.append(train)\n",
    "            val_loss.append(val)\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch_losses': epoch_losses,\n",
    "                'last_file': None,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, filename = checkpoint_path)\n",
    "        end = time.time()\n",
    "        epoch_train_loss = sum(train_loss)/len(train_loss)\n",
    "        epoch_val_loss = sum(val_loss)/len(val_loss)\n",
    "        epoch_losses.append((epoch_train_loss, epoch_val_loss))\n",
    "        print(\"Epoch:{} Time:{:.2f} Train Loss:{:.4f} Val Loss:{:.4f}\".format(epoch, end-start, epoch_train_loss, epoch_val_loss))\n",
    "        if early_stopping(epoch_val_loss):\n",
    "            print(\"Early Stopping at Epoch:{}\".format(epoch))\n",
    "            break\n",
    "    return model, epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstmdecoder(device, dataset_path = DEFAULT_ENCODED_DATASET_FOLDER, checkpoint_path = DEFAULT_LSTMDECODER_FILE):\n",
    "    model, epoch_losses = run_training(dataset_path, device, checkpoint_path)\n",
    "    model.eval()\n",
    "    torch.save(model.state_dict(), \"model/lstmdecoder.pth\")\n",
    "    torch.save(model.lstm.state_dict(), \"model/lstm.pth\")\n",
    "    torch.save(model.decoder.state_dict(), \"model/ctnn3d.pth\")\n",
    "    return model, epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint 'lstmdecoder.pth.tar'\n",
      "[0.03972855330828358] [0.03484101966023445]\n",
      "Loaded checkpoint 'lstmdecoder.pth.tar' (epoch 0)\n",
      "dataset/1b00e4c41b4195807e1c97634acf0214\n",
      "dataset/1a9bc7dd64da10f344ebbc705ad8c07\n",
      "dataset/1b2a8980c17fdd97c897e89b561760b1\n",
      "dataset/1a2f00c6886f14354d85fb76de815344\n",
      "dataset/1a2b1863733c2ca65e26ee427f1e5a4c\n",
      "dataset/1abf3b20f05ed8ea902d6f4ac8edb5f4\n",
      "dataset/1b1cf4f2cc24a2a2a5895e3729304f68\n",
      "dataset/1b00f29471a41f59e92b1dc10fc46551\n",
      "Epoch:0 Train Size:153 Val Size:39\n",
      "Epoch:0 Sub Train Time:31.84 Train Loss:0.0325 Val Loss:0.0219\n",
      "Epoch:0 Time:32.76 Train Loss:0.0325 Val Loss:0.0219\n",
      "dataset/1b00e4c41b4195807e1c97634acf0214\n",
      "dataset/1a9bc7dd64da10f344ebbc705ad8c07\n",
      "dataset/1b2a8980c17fdd97c897e89b561760b1\n",
      "dataset/1a2f00c6886f14354d85fb76de815344\n",
      "dataset/1a2b1863733c2ca65e26ee427f1e5a4c\n",
      "dataset/1abf3b20f05ed8ea902d6f4ac8edb5f4\n",
      "dataset/1b1cf4f2cc24a2a2a5895e3729304f68\n",
      "dataset/1b00f29471a41f59e92b1dc10fc46551\n",
      "Epoch:1 Train Size:153 Val Size:39\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_441702/3958627068.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lstmdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_441702/2704631956.py\u001b[0m in \u001b[0;36mtrain_lstmdecoder\u001b[0;34m(device, dataset_path, checkpoint_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_lstmdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULT_ENCODED_DATASET_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULT_LSTMDECODER_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model/lstmdecoder.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model/lstm.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_441702/599937799.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(file_path, device, checkpoint_path)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoxels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_sub_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_441702/1603289631.py\u001b[0m in \u001b[0;36mtrain_sub_epoch\u001b[0;34m(epoch, datas, model, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_chennal\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtrain_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    250\u001b[0m                  \u001b[0mfused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fused'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                  \u001b[0mgrad_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                  found_inf=found_inf)\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    314\u001b[0m          \u001b[0mdifferentiable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdifferentiable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m          \u001b[0mgrad_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m          found_inf=found_inf)\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model, epoch_losses = train_lstmdecoder(device)\n",
    "    print(epoch_losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "model = LSTM()\n",
    "batch_size = 1\n",
    "\n",
    "# 初始化隐藏层状态\n",
    "h_0 = torch.zeros(model.num_layers, batch_size, model.hidden_size)\n",
    "c_0 = torch.zeros(model.num_layers, batch_size, model.hidden_size)\n",
    "\n",
    "# 第一次输入\n",
    "input_1 = torch.randn(batch_size, 1, LSTM_NEUROES)  # (batch_size, seq_len, input_size)\n",
    "print(input_1.shape)\n",
    "output, h_n, c_n = model(input_1, None, h_0, c_0)\n",
    "print(\"第一次输出:\", output)\n",
    "\n",
    "# 第二次输入，使用第一次的输出作为一部分输入\n",
    "input_2 = torch.randn(batch_size, 1, LSTM_NEUROES)  # 另外的输入\n",
    "new_input = torch.cat((input_2, output.unsqueeze(1)), dim=2)  # 合并输出作为输入的一部分\n",
    "new_input = model.input_fc(new_input)  # 映射到原始input_size\n",
    "\n",
    "output, h_n, c_n = model(new_input, h_n, c_n)\n",
    "print(\"第二次输出:\", output)\n",
    "\n",
    "# 多次迭代输入进行修正\n",
    "num_iterations = 5\n",
    "for i in range(num_iterations):\n",
    "    input_next = torch.randn(batch_size, 1, LSTM_NEUROES)  # 另外的输入\n",
    "    new_input = torch.cat((input_next, output.unsqueeze(1)), dim=2)  # 合并输出作为输入的一部分\n",
    "    new_input = model.input_fc(new_input)  # 映射到原始input_size\n",
    "\n",
    "    output, h_n, c_n = model(new_input, h_n, c_n)\n",
    "    print(f\"第{i+3}次输出:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
