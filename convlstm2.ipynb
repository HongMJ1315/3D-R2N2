{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 - Hidden state h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "f: torch.Size([1, 8, 15, 15, 15]), i: torch.Size([1, 8, 15, 15, 15]), o: torch.Size([1, 8, 15, 15, 15]), g: torch.Size([1, 8, 15, 15, 15]), c_cur: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Output h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "Output at time step 0: torch.Size([1, 8, 15, 15, 15])\n",
      "Hidden state at time step 0: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Hidden state h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "f: torch.Size([1, 8, 15, 15, 15]), i: torch.Size([1, 8, 15, 15, 15]), o: torch.Size([1, 8, 15, 15, 15]), g: torch.Size([1, 8, 15, 15, 15]), c_cur: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Output h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "Output at time step 1: torch.Size([1, 8, 15, 15, 15])\n",
      "Hidden state at time step 1: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Hidden state h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "f: torch.Size([1, 8, 15, 15, 15]), i: torch.Size([1, 8, 15, 15, 15]), o: torch.Size([1, 8, 15, 15, 15]), g: torch.Size([1, 8, 15, 15, 15]), c_cur: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Output h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "Output at time step 2: torch.Size([1, 8, 15, 15, 15])\n",
      "Hidden state at time step 2: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Hidden state h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "f: torch.Size([1, 8, 15, 15, 15]), i: torch.Size([1, 8, 15, 15, 15]), o: torch.Size([1, 8, 15, 15, 15]), g: torch.Size([1, 8, 15, 15, 15]), c_cur: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Output h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "Output at time step 3: torch.Size([1, 8, 15, 15, 15])\n",
      "Hidden state at time step 3: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Hidden state h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "f: torch.Size([1, 8, 15, 15, 15]), i: torch.Size([1, 8, 15, 15, 15]), o: torch.Size([1, 8, 15, 15, 15]), g: torch.Size([1, 8, 15, 15, 15]), c_cur: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Output h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "Output at time step 4: torch.Size([1, 8, 15, 15, 15])\n",
      "Hidden state at time step 4: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Hidden state h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "f: torch.Size([1, 8, 15, 15, 15]), i: torch.Size([1, 8, 15, 15, 15]), o: torch.Size([1, 8, 15, 15, 15]), g: torch.Size([1, 8, 15, 15, 15]), c_cur: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Output h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "Output at time step 5: torch.Size([1, 8, 15, 15, 15])\n",
      "Hidden state at time step 5: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Hidden state h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "f: torch.Size([1, 8, 15, 15, 15]), i: torch.Size([1, 8, 15, 15, 15]), o: torch.Size([1, 8, 15, 15, 15]), g: torch.Size([1, 8, 15, 15, 15]), c_cur: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Output h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "Output at time step 6: torch.Size([1, 8, 15, 15, 15])\n",
      "Hidden state at time step 6: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Hidden state h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "f: torch.Size([1, 8, 15, 15, 15]), i: torch.Size([1, 8, 15, 15, 15]), o: torch.Size([1, 8, 15, 15, 15]), g: torch.Size([1, 8, 15, 15, 15]), c_cur: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Output h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "Output at time step 7: torch.Size([1, 8, 15, 15, 15])\n",
      "Hidden state at time step 7: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Hidden state h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "f: torch.Size([1, 8, 15, 15, 15]), i: torch.Size([1, 8, 15, 15, 15]), o: torch.Size([1, 8, 15, 15, 15]), g: torch.Size([1, 8, 15, 15, 15]), c_cur: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Output h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "Output at time step 8: torch.Size([1, 8, 15, 15, 15])\n",
      "Hidden state at time step 8: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Hidden state h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "f: torch.Size([1, 8, 15, 15, 15]), i: torch.Size([1, 8, 15, 15, 15]), o: torch.Size([1, 8, 15, 15, 15]), g: torch.Size([1, 8, 15, 15, 15]), c_cur: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Output h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "Output at time step 9: torch.Size([1, 8, 15, 15, 15])\n",
      "Hidden state at time step 9: torch.Size([1, 8, 15, 15, 15])\n",
      "Final Output: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Hidden state h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_len, input_dim, hidden_dim, kernel_size, bias):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_len = input_len\n",
    "        self.fc = nn.Linear(input_len, input_dim * kernel_size[0] * kernel_size[1] * kernel_size[2])\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding_size = (kernel_size[0] // 2, kernel_size[1] // 2, kernel_size[2] // 2)\n",
    "        if any(k % 2 == 0 for k in kernel_size):\n",
    "            raise ValueError(\"Only support odd kernel size\")\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv3d(self.input_dim + self.hidden_dim, \n",
    "                              4 * self.hidden_dim,  # 4* 是因为后面输出时要切4片\n",
    "                              self.kernel_size, \n",
    "                              padding=self.padding_size, \n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        \n",
    "        # 经过全连接层\n",
    "        input_tensor = input_tensor.view(-1, self.input_len)\n",
    "        input_fc = self.fc(input_tensor)\n",
    "        \n",
    "        # 变形\n",
    "        batch_size = input_tensor.size(0)\n",
    "        depth, height, width = h_cur.size(2), h_cur.size(3), h_cur.size(4)\n",
    "        input_fc = input_fc.view(batch_size, self.input_dim, depth, height, width)\n",
    "        \n",
    "        combined = torch.cat((input_fc, h_cur), dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_f, cc_i, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        print(\"f: {}, i: {}, o: {}, g: {}, c_cur: {}\".format(f.shape, i.shape, o.shape, g.shape, c_cur.shape))\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        depth, height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, depth, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, depth, height, width, device=self.conv.weight.device))\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_len, input_dim, hidden_dim, kernel_size, num_layers, image_size, bias=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.input_len = input_len\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.image_size = image_size  # 添加 image_size\n",
    "        self.bias = bias\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, num_layers):\n",
    "            cur_input_dim = input_dim if i == 0 else self.hidden_dim[i-1]\n",
    "            cell_list.append(ConvLSTMCell(input_len=self.input_len,\n",
    "                                          input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size,\n",
    "                                          bias=self.bias))\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state):\n",
    "        cur_layer_input = input_tensor\n",
    "        new_hidden_state = []\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            print(f\"Layer {layer_idx} - Hidden state h: {h.shape}, c: {c.shape}\")\n",
    "            h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input, cur_state=[h, c])\n",
    "            print(f\"Layer {layer_idx} - Output h: {h.shape}, c: {c.shape}\")\n",
    "            cur_layer_input = h\n",
    "            new_hidden_state.append([h, c])\n",
    "\n",
    "        return cur_layer_input, new_hidden_state\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return [cell.init_hidden(batch_size, self.image_size) for cell in self.cell_list]\n",
    "\n",
    "# 测试代码\n",
    "input_len = 3000  # 输入长度\n",
    "input_dim = 6\n",
    "hidden_dim = [8]\n",
    "kernel_size = (15, 15, 15)\n",
    "num_layers = 1\n",
    "image_size = (15, 15, 15)  # 这里假设卷积核大小为3x3x3\n",
    "\n",
    "model = ConvLSTM(input_len=input_len,\n",
    "                 input_dim=input_dim,\n",
    "                 hidden_dim=hidden_dim,\n",
    "                 kernel_size=kernel_size,\n",
    "                 num_layers=num_layers,\n",
    "                 image_size=image_size,  # 传递 image_size\n",
    "                 bias=True)\n",
    "\n",
    "# 单个时间步的一维数据\n",
    "single_time_step_data = torch.randn(1, input_len)\n",
    "hidden_state = model.init_hidden(batch_size=1)\n",
    "\n",
    "# 模拟处理时间序列数据\n",
    "seq_len = 10\n",
    "for t in range(seq_len):\n",
    "    output, hidden_state = model(single_time_step_data, hidden_state)\n",
    "    print(f\"Output at time step {t}: {output.shape}\")\n",
    "    print(f\"Hidden state at time step {t}: {hidden_state[0][0].shape}\")\n",
    "\n",
    "# 打印最后一个时间步的输出\n",
    "print(f\"Final Output: {output.shape}\")\n",
    "\n",
    "# 打印最后一个时间步的隐藏状态\n",
    "for layer_idx, (h, c) in enumerate(hidden_state):\n",
    "    print(f\"Layer {layer_idx} - Hidden state h: {h.shape}, c: {c.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 - Hidden state h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "f: torch.Size([1, 8, 15, 15, 15]), i: torch.Size([1, 8, 15, 15, 15]), o: torch.Size([1, 8, 15, 15, 15]), g: torch.Size([1, 8, 15, 15, 15]), c_cur: torch.Size([1, 8, 15, 15, 15])\n",
      "Layer 0 - Output h: torch.Size([1, 8, 15, 15, 15]), c: torch.Size([1, 8, 15, 15, 15])\n",
      "Output: torch.Size([1, 4, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "LSTM_NEUROES = 3000\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_len, input_dim, hidden_dim, kernel_size, bias):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_len = input_len\n",
    "        self.fc = nn.Linear(input_len, input_dim * kernel_size[0] * kernel_size[1] * kernel_size[2])\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding_size = (kernel_size[0] // 2, kernel_size[1] // 2, kernel_size[2] // 2)\n",
    "        if any(k % 2 == 0 for k in kernel_size):\n",
    "            raise ValueError(\"Only support odd kernel size\")\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv3d(self.input_dim + self.hidden_dim, \n",
    "                              4 * self.hidden_dim,  # 4* 是因为后面输出时要切4片\n",
    "                              self.kernel_size, \n",
    "                              padding=self.padding_size, \n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        \n",
    "        # 经过全连接层\n",
    "        input_tensor = input_tensor.view(-1, self.input_len)\n",
    "        input_fc = self.fc(input_tensor)\n",
    "        \n",
    "        # 变形\n",
    "        batch_size = input_tensor.size(0)\n",
    "        depth, height, width = h_cur.size(2), h_cur.size(3), h_cur.size(4)\n",
    "        input_fc = input_fc.view(batch_size, self.input_dim, depth, height, width)\n",
    "        \n",
    "        combined = torch.cat((input_fc, h_cur), dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_f, cc_i, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        print(\"f: {}, i: {}, o: {}, g: {}, c_cur: {}\".format(f.shape, i.shape, o.shape, g.shape, c_cur.shape))\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        depth, height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, depth, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, depth, height, width, device=self.conv.weight.device))\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_len, input_dim, hidden_dim, kernel_size, num_layers, image_size, bias=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.input_len = input_len\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.image_size = image_size  # 添加 image_size\n",
    "        self.bias = bias\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, num_layers):\n",
    "            cur_input_dim = input_dim if i == 0 else self.hidden_dim[i-1]\n",
    "            cell_list.append(ConvLSTMCell(input_len=self.input_len,\n",
    "                                          input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size,\n",
    "                                          bias=self.bias))\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state):\n",
    "        cur_layer_input = input_tensor\n",
    "        new_hidden_state = []\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            print(f\"Layer {layer_idx} - Hidden state h: {h.shape}, c: {c.shape}\")\n",
    "            h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input, cur_state=[h, c])\n",
    "            print(f\"Layer {layer_idx} - Output h: {h.shape}, c: {c.shape}\")\n",
    "            cur_layer_input = h\n",
    "            new_hidden_state.append([h, c])\n",
    "\n",
    "        return cur_layer_input, new_hidden_state\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return [cell.init_hidden(batch_size, self.image_size) for cell in self.cell_list]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "class CTNN3DDecoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CTNN3DDecoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose3d(in_channels=input_dim, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.dropout1 = nn.Dropout3d(p=0.25)\n",
    "        \n",
    "        self.conv2 = nn.ConvTranspose3d(in_channels=12, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.dropout2 = nn.Dropout3d(p=0.25)\n",
    "        \n",
    "        self.conv3 = nn.ConvTranspose3d(in_channels=8, out_channels=4, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.dropout3 = nn.Dropout3d(p=0.25)\n",
    "        \n",
    "        self.conv4 = nn.ConvTranspose3d(in_channels=4, out_channels=4, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.upsample4 = nn.Upsample(size=(32, 32, 32), mode='nearest')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.upsample1(out)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.upsample2(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.upsample3(out)\n",
    "        out = self.dropout3(out)\n",
    "        \n",
    "        out = self.conv4(out)\n",
    "        out = self.relu4(out)\n",
    "        out = self.upsample4(out)\n",
    "        return out\n",
    "    \n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, input_len, input_dim, hidden_dim, kernel_size, num_layers, image_size, bias=False):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.lstm = ConvLSTM(input_len, input_dim, hidden_dim, kernel_size, num_layers, image_size, bias)\n",
    "        self.decoder = CTNN3DDecoder(hidden_dim[0])\n",
    "        \n",
    "    def forward(self, x, hidden_state):\n",
    "        out, hidden_state = self.lstm(x, hidden_state)\n",
    "        out = self.decoder(out)\n",
    "        return out, hidden_state\n",
    "    \n",
    "hidden_dim = [8]\n",
    "kernel_size = (15, 15, 15)\n",
    "num_layers = 1\n",
    "image_size = (15, 15, 15)  # 这里假设卷积核大小为3x3x3\n",
    "single_time_step_data = torch.randn(1, input_len)\n",
    "\n",
    "model = LSTMDecoder(input_len=input_len, input_dim=16, hidden_dim=hidden_dim, kernel_size=kernel_size, num_layers=num_layers, image_size=image_size, bias=True)\n",
    "hidden_state = model.lstm.init_hidden(batch_size=1)\n",
    "output, hidden_state = model(single_time_step_data, hidden_state)\n",
    "print(f\"Output: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
